{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teddy-teem/deep-learning/blob/master/Hyperparameter_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueJxsRjDmR-Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtXXmxn1na_j",
        "outputId": "24588b1a-21f9-4e71-f5b5-8fb18911ee55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFfvmjF_nHkt",
        "outputId": "ae1d92e6-b6eb-41b6-a4fe-34f1667529f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ee99c3ecdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vebLwheBVm8",
        "outputId": "42b7c249-a66d-4b5e-8b81-255ff6c5904d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/Datasets/dataset-MNIST-fashion/mnist_fashion_train.csv'"
      ],
      "metadata": {
        "id": "5TMYwlnMBlQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(file_path)\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yklrkoVvoUrb",
        "outputId": "c9e07aae-4db2-4a4b-c82b-129e48c4550d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(4,4, figsize=(10, 10))\n",
        "fig.suptitle(\"first 16 images\", fontsize=16)\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = df.iloc[i, 1:].values.reshape(28,28)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(df.iloc[i, 0])\n",
        "    ax.imshow(img, cmap='gray')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "I8WU5FvipoV9",
        "outputId": "9ad040a2-857a-4a72-da01-c41365163e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 1:].values\n",
        "y = df.iloc[:, 0].values"
      ],
      "metadata": {
        "id": "orrXileNp1Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vPx8MTYQp31y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For better tuning, the pixel value should be between 0 to 1 that why we devided by 255 each\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "metadata": {
        "id": "0VmeNo8dp6Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features =torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.features[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)"
      ],
      "metadata": {
        "id": "4WHi-I71p8H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "test_dataset = CustomDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "hau-exSpp-lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZyGYqgJqA9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNN(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "    super().__init__()\n",
        "\n",
        "    layers = []\n",
        "    for i in range(num_hidden_layers):\n",
        "      layers.append(nn.Linear(input_dim, neurons_per_layer))\n",
        "      layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(nn.Dropout(dropout_rate))\n",
        "      input_dim = neurons_per_layer\n",
        "    layers.append(nn.Linear(neurons_per_layer, output_dim))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n"
      ],
      "metadata": {
        "id": "k_iCpmqey_Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# objective function\n",
        "\n",
        "def objective(trial):\n",
        "  # next trial's Hyperparameters values from the search space\n",
        "  num_hidden_a = trial.suggest_int(\"num_hidden_layers\", 1, 5)\n",
        "  neurons_per_layer = trial.suggest_int(\"neurons_per_layer\", 8, 128, step=8)\n",
        "  epochs = trial.suggest_int(\"epochs\", 10, 100, step=10)\n",
        "  learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
        "  dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5, step=0.1)\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n",
        "  weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
        "\n",
        "  # data loader\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True) # pin_memory=True to speed up the process\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  # model init\n",
        "  input_dim = 784\n",
        "  output_dim = 10\n",
        "  model = MyNN(input_dim, output_dim, num_hidden_a, neurons_per_layer, dropout_rate)\n",
        "  model.to(device)\n",
        "\n",
        "  #optimizer selection\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "  if optimizer_name == \"Adam\":\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "  elif optimizer_name == \"RMSprop\":\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "  else:\n",
        "    optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "  # training loop\n",
        "  for epoch in range(epochs):\n",
        "    total_epoch_loss = 0\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "     batch_features = batch_features.to(device)\n",
        "     batch_labels = batch_labels.to(device)\n",
        "     out = model(batch_features)\n",
        "     loss = criterion(out, batch_labels)\n",
        "     optimizer.zero_grad()\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "\n",
        "     total_epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_epoch_loss/len(train_loader)}\")\n",
        "\n",
        "  # evaluation\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_features, batch_labels in test_loader:\n",
        "      batch_features = batch_features.to(device)\n",
        "      batch_labels = batch_labels.to(device)\n",
        "      out = model(batch_features)\n",
        "      _, predicted = torch.max(out.data, 1)\n",
        "      total += batch_labels.shape[0]\n",
        "      correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "  accuracy=100 * correct / total\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "T62dd1D8x-Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeXYg-BU2wM3",
        "outputId": "16edec91-bfc2-4ac8-968d-b7f0c1160cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiizzYWG20dp",
        "outputId": "2a3bee96-ef8b-4d6e-d21f-1ffd1ae7e29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-15 17:30:44,791] A new study created in memory with name: no-name-bf3aa9b5-e0fd-499b-91cd-b6987893f67c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.optimize(objective, n_trials=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1mA8WWM-3DE2",
        "outputId": "e8538147-605c-46e6-f439-75c4a41c7b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.7782278867562612\n",
            "Epoch 2, Loss: 0.6410791554848353\n",
            "Epoch 3, Loss: 0.6176591178973516\n",
            "Epoch 4, Loss: 0.6062265564997991\n",
            "Epoch 5, Loss: 0.6052949308951696\n",
            "Epoch 6, Loss: 0.6039797999461491\n",
            "Epoch 7, Loss: 0.6032202521165212\n",
            "Epoch 8, Loss: 0.6032015809218089\n",
            "Epoch 9, Loss: 0.595425909837087\n",
            "Epoch 10, Loss: 0.6085987553596497\n",
            "Epoch 11, Loss: 0.6019191140731176\n",
            "Epoch 12, Loss: 0.6019335836569468\n",
            "Epoch 13, Loss: 0.5964481815894445\n",
            "Epoch 14, Loss: 0.5958421780665716\n",
            "Epoch 15, Loss: 0.5980685529311498\n",
            "Epoch 16, Loss: 0.5972554384668668\n",
            "Epoch 17, Loss: 0.5992592494090399\n",
            "Epoch 18, Loss: 0.5925921991268793\n",
            "Epoch 19, Loss: 0.5951510524749756\n",
            "Epoch 20, Loss: 0.5955349061489105\n",
            "Epoch 21, Loss: 0.5944489797353745\n",
            "Epoch 22, Loss: 0.592289040406545\n",
            "Epoch 23, Loss: 0.5925712291399637\n",
            "Epoch 24, Loss: 0.5929480526447296\n",
            "Epoch 25, Loss: 0.5931064592202504\n",
            "Epoch 26, Loss: 0.5950933369000753\n",
            "Epoch 27, Loss: 0.5943992225726445\n",
            "Epoch 28, Loss: 0.5938462136983872\n",
            "Epoch 29, Loss: 0.5879669995307922\n",
            "Epoch 30, Loss: 0.5896714668273926\n",
            "Epoch 31, Loss: 0.595701909383138\n",
            "Epoch 32, Loss: 0.5948191653092703\n",
            "Epoch 33, Loss: 0.5972140419085821\n",
            "Epoch 34, Loss: 0.5945438286860784\n",
            "Epoch 35, Loss: 0.595561061779658\n",
            "Epoch 36, Loss: 0.5946645580132802\n",
            "Epoch 37, Loss: 0.589313379406929\n",
            "Epoch 38, Loss: 0.5925796833435695\n",
            "Epoch 39, Loss: 0.586994623541832\n",
            "Epoch 40, Loss: 0.5961540583372116\n",
            "Epoch 41, Loss: 0.5881604973077774\n",
            "Epoch 42, Loss: 0.5923868563175202\n",
            "Epoch 43, Loss: 0.5922006100813548\n",
            "Epoch 44, Loss: 0.593566388964653\n",
            "Epoch 45, Loss: 0.5889528189102808\n",
            "Epoch 46, Loss: 0.5923001539707183\n",
            "Epoch 47, Loss: 0.5887312077283859\n",
            "Epoch 48, Loss: 0.5925819600820541\n",
            "Epoch 49, Loss: 0.5951857933998108\n",
            "Epoch 50, Loss: 0.5924231377442678\n",
            "Epoch 51, Loss: 0.5883305685917536\n",
            "Epoch 52, Loss: 0.5889902345339457\n",
            "Epoch 53, Loss: 0.592480308453242\n",
            "Epoch 54, Loss: 0.589559987783432\n",
            "Epoch 55, Loss: 0.584022675593694\n",
            "Epoch 56, Loss: 0.591460005402565\n",
            "Epoch 57, Loss: 0.5924884341557821\n",
            "Epoch 58, Loss: 0.5908539913892746\n",
            "Epoch 59, Loss: 0.591943542877833\n",
            "Epoch 60, Loss: 0.5922741226752599\n",
            "Epoch 61, Loss: 0.5861659060716629\n",
            "Epoch 62, Loss: 0.5907221464713415\n",
            "Epoch 63, Loss: 0.5847655472358068\n",
            "Epoch 64, Loss: 0.5878299748500189\n",
            "Epoch 65, Loss: 0.590116201321284\n",
            "Epoch 66, Loss: 0.5877368909120559\n",
            "Epoch 67, Loss: 0.5912797145843506\n",
            "Epoch 68, Loss: 0.5879998976389567\n",
            "Epoch 69, Loss: 0.5929727646509806\n",
            "Epoch 70, Loss: 0.5919356408913931\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-07-15 17:33:29,487] Trial 0 finished with value: 83.48333333333333 and parameters: {'num_hidden_layers': 3, 'neurons_per_layer': 64, 'epochs': 70, 'learning_rate': 0.008379743001239661, 'dropout_rate': 0.4, 'batch_size': 64, 'optimizer': 'Adam', 'weight_decay': 1.6487773520205885e-05}. Best is trial 0 with value: 83.48333333333333.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.68203533522288\n",
            "Epoch 2, Loss: 1.110332128047943\n",
            "Epoch 3, Loss: 0.8655115184783936\n",
            "Epoch 4, Loss: 0.7315633467833201\n",
            "Epoch 5, Loss: 0.6414402026335398\n",
            "Epoch 6, Loss: 0.588910197575887\n",
            "Epoch 7, Loss: 0.5507334023316701\n",
            "Epoch 8, Loss: 0.5194689134756724\n",
            "Epoch 9, Loss: 0.4989699157079061\n",
            "Epoch 10, Loss: 0.4826442914009094\n",
            "Epoch 11, Loss: 0.4701685107549032\n",
            "Epoch 12, Loss: 0.4605274725755056\n",
            "Epoch 13, Loss: 0.44493602856000264\n",
            "Epoch 14, Loss: 0.4353729114532471\n",
            "Epoch 15, Loss: 0.42896949529647826\n",
            "Epoch 16, Loss: 0.4252572671175003\n",
            "Epoch 17, Loss: 0.41500637884934743\n",
            "Epoch 18, Loss: 0.4111407713095347\n",
            "Epoch 19, Loss: 0.40153336675961815\n",
            "Epoch 20, Loss: 0.40028621125221253\n",
            "Epoch 21, Loss: 0.39687288530667625\n",
            "Epoch 22, Loss: 0.3906432868242264\n",
            "Epoch 23, Loss: 0.38741612728436786\n",
            "Epoch 24, Loss: 0.38602019123236336\n",
            "Epoch 25, Loss: 0.3774867013692856\n",
            "Epoch 26, Loss: 0.3778787015279134\n",
            "Epoch 27, Loss: 0.37420644696553546\n",
            "Epoch 28, Loss: 0.36934018894036613\n",
            "Epoch 29, Loss: 0.3674074698289235\n",
            "Epoch 30, Loss: 0.36516347984472913\n",
            "Epoch 31, Loss: 0.3612555159330368\n",
            "Epoch 32, Loss: 0.36025221927960716\n",
            "Epoch 33, Loss: 0.3554836405913035\n",
            "Epoch 34, Loss: 0.3520700528224309\n",
            "Epoch 35, Loss: 0.35373833338419597\n",
            "Epoch 36, Loss: 0.34759589258829754\n",
            "Epoch 37, Loss: 0.34694695794582364\n",
            "Epoch 38, Loss: 0.3422765394846598\n",
            "Epoch 39, Loss: 0.34088720635573067\n",
            "Epoch 40, Loss: 0.33793013588587445\n",
            "Epoch 41, Loss: 0.33213620515664416\n",
            "Epoch 42, Loss: 0.33494427474339805\n",
            "Epoch 43, Loss: 0.33124396856625876\n",
            "Epoch 44, Loss: 0.331528023203214\n",
            "Epoch 45, Loss: 0.3308104863166809\n",
            "Epoch 46, Loss: 0.32386110329627993\n",
            "Epoch 47, Loss: 0.3242498954137166\n",
            "Epoch 48, Loss: 0.32325705587863923\n",
            "Epoch 49, Loss: 0.32077271401882174\n",
            "Epoch 50, Loss: 0.31831560397148134\n",
            "Epoch 51, Loss: 0.31611799440781274\n",
            "Epoch 52, Loss: 0.32223307160536446\n",
            "Epoch 53, Loss: 0.3150702492396037\n",
            "Epoch 54, Loss: 0.3143892817894618\n",
            "Epoch 55, Loss: 0.31307802780469257\n",
            "Epoch 56, Loss: 0.3079551203250885\n",
            "Epoch 57, Loss: 0.306187153895696\n",
            "Epoch 58, Loss: 0.3087947394053141\n",
            "Epoch 59, Loss: 0.3042871916294098\n",
            "Epoch 60, Loss: 0.3036407228708267\n",
            "Epoch 61, Loss: 0.3016423333088557\n",
            "Epoch 62, Loss: 0.29958205338319144\n",
            "Epoch 63, Loss: 0.3004108722607295\n",
            "Epoch 64, Loss: 0.29436279634634654\n",
            "Epoch 65, Loss: 0.2948510996500651\n",
            "Epoch 66, Loss: 0.29210958524545033\n",
            "Epoch 67, Loss: 0.29442482566833494\n",
            "Epoch 68, Loss: 0.28953873364130656\n",
            "Epoch 69, Loss: 0.2912276033560435\n",
            "Epoch 70, Loss: 0.2881013695398966\n",
            "Epoch 71, Loss: 0.2858364002307256\n",
            "Epoch 72, Loss: 0.2851531850496928\n",
            "Epoch 73, Loss: 0.28622881722450255\n",
            "Epoch 74, Loss: 0.28185399305820463\n",
            "Epoch 75, Loss: 0.27887020401159923\n",
            "Epoch 76, Loss: 0.27961390823125837\n",
            "Epoch 77, Loss: 0.2860883968671163\n",
            "Epoch 78, Loss: 0.2804118113120397\n",
            "Epoch 79, Loss: 0.278662998020649\n",
            "Epoch 80, Loss: 0.27511444421609244\n",
            "Epoch 81, Loss: 0.2746851230462392\n",
            "Epoch 82, Loss: 0.27585531014204023\n",
            "Epoch 83, Loss: 0.2753730039993922\n",
            "Epoch 84, Loss: 0.2734937932093938\n",
            "Epoch 85, Loss: 0.2692369828224182\n",
            "Epoch 86, Loss: 0.2704122080405553\n",
            "Epoch 87, Loss: 0.26802227964003883\n",
            "Epoch 88, Loss: 0.2653955484628677\n",
            "Epoch 89, Loss: 0.2699374142885208\n",
            "Epoch 90, Loss: 0.26801692390441895\n",
            "Epoch 91, Loss: 0.26557838813463847\n",
            "Epoch 92, Loss: 0.26534450745582583\n",
            "Epoch 93, Loss: 0.26261688657601673\n",
            "Epoch 94, Loss: 0.26239798571666084\n",
            "Epoch 95, Loss: 0.26045756149291993\n",
            "Epoch 96, Loss: 0.25677912000815073\n",
            "Epoch 97, Loss: 0.2598503915866216\n",
            "Epoch 98, Loss: 0.2585292003949483\n",
            "Epoch 99, Loss: 0.25780211756626764\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-07-15 17:35:58,750] Trial 1 finished with value: 88.80833333333334 and parameters: {'num_hidden_layers': 4, 'neurons_per_layer': 72, 'epochs': 100, 'learning_rate': 5.817284058149812e-05, 'dropout_rate': 0.2, 'batch_size': 128, 'optimizer': 'Adam', 'weight_decay': 3.794627897715172e-05}. Best is trial 1 with value: 88.80833333333334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Loss: 0.255396359761556\n",
            "Epoch 1, Loss: 2.4136629158655802\n",
            "Epoch 2, Loss: 2.366573638598124\n",
            "Epoch 3, Loss: 2.3332200090090436\n",
            "Epoch 4, Loss: 2.30971692721049\n",
            "Epoch 5, Loss: 2.2818051891326903\n",
            "Epoch 6, Loss: 2.260985142787298\n",
            "Epoch 7, Loss: 2.237679481903712\n",
            "Epoch 8, Loss: 2.215232809861501\n",
            "Epoch 9, Loss: 2.1962071481545764\n",
            "Epoch 10, Loss: 2.172971254825592\n",
            "Epoch 11, Loss: 2.1526389748255412\n",
            "Epoch 12, Loss: 2.13418150806427\n",
            "Epoch 13, Loss: 2.1133041189511617\n",
            "Epoch 14, Loss: 2.0908022828102113\n",
            "Epoch 15, Loss: 2.074731256246567\n",
            "Epoch 16, Loss: 2.0577516024112703\n",
            "Epoch 17, Loss: 2.039635686079661\n",
            "Epoch 18, Loss: 2.023580117146174\n",
            "Epoch 19, Loss: 2.002714350382487\n",
            "Epoch 20, Loss: 1.9882828198273976\n",
            "Epoch 21, Loss: 1.9741148099899293\n",
            "Epoch 22, Loss: 1.9572885111172993\n",
            "Epoch 23, Loss: 1.941612559636434\n",
            "Epoch 24, Loss: 1.931213763554891\n",
            "Epoch 25, Loss: 1.91318346007665\n",
            "Epoch 26, Loss: 1.8995763202508291\n",
            "Epoch 27, Loss: 1.8831851626237233\n",
            "Epoch 28, Loss: 1.8693916906515757\n",
            "Epoch 29, Loss: 1.859006135861079\n",
            "Epoch 30, Loss: 1.843607172727585\n",
            "Epoch 31, Loss: 1.8352126173178356\n",
            "Epoch 32, Loss: 1.8191371581554412\n",
            "Epoch 33, Loss: 1.813045936425527\n",
            "Epoch 34, Loss: 1.8034488998254141\n",
            "Epoch 35, Loss: 1.7929294262727102\n",
            "Epoch 36, Loss: 1.78227543314298\n",
            "Epoch 37, Loss: 1.772243244489034\n",
            "Epoch 38, Loss: 1.758614423116048\n",
            "Epoch 39, Loss: 1.7511843570073446\n",
            "Epoch 40, Loss: 1.7418930269082387\n",
            "Epoch 41, Loss: 1.734869925737381\n",
            "Epoch 42, Loss: 1.7288845029671986\n",
            "Epoch 43, Loss: 1.7169520611763\n",
            "Epoch 44, Loss: 1.7108103968302408\n",
            "Epoch 45, Loss: 1.7013573151429495\n",
            "Epoch 46, Loss: 1.6927985691229501\n",
            "Epoch 47, Loss: 1.6876997186342875\n",
            "Epoch 48, Loss: 1.683197944800059\n",
            "Epoch 49, Loss: 1.676492725133896\n",
            "Epoch 50, Loss: 1.6681714820067088\n",
            "Epoch 51, Loss: 1.6601321201324464\n",
            "Epoch 52, Loss: 1.6546988881429037\n",
            "Epoch 53, Loss: 1.6509750400384267\n",
            "Epoch 54, Loss: 1.6411813782850901\n",
            "Epoch 55, Loss: 1.6311517647107443\n",
            "Epoch 56, Loss: 1.6305429777304332\n",
            "Epoch 57, Loss: 1.6254113593101502\n",
            "Epoch 58, Loss: 1.6241800000667572\n",
            "Epoch 59, Loss: 1.6112145838737488\n",
            "Epoch 60, Loss: 1.6093223311106364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-15 17:40:56,939] Trial 2 finished with value: 48.125 and parameters: {'num_hidden_layers': 5, 'neurons_per_layer': 56, 'epochs': 60, 'learning_rate': 0.0001371131333415193, 'dropout_rate': 0.5, 'batch_size': 32, 'optimizer': 'SGD', 'weight_decay': 5.8628045467451346e-05}. Best is trial 1 with value: 88.80833333333334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.1402696471214293\n",
            "Epoch 2, Loss: 1.7433594827651977\n",
            "Epoch 3, Loss: 1.549862916469574\n",
            "Epoch 4, Loss: 1.429688747326533\n",
            "Epoch 5, Loss: 1.3440118475755056\n",
            "Epoch 6, Loss: 1.2731859405040742\n",
            "Epoch 7, Loss: 1.2241672022342682\n",
            "Epoch 8, Loss: 1.1760893087387085\n",
            "Epoch 9, Loss: 1.1350024768511455\n",
            "Epoch 10, Loss: 1.1020866539080938\n",
            "Epoch 11, Loss: 1.0740418504476548\n",
            "Epoch 12, Loss: 1.0486737167040507\n",
            "Epoch 13, Loss: 1.0238330425024034\n",
            "Epoch 14, Loss: 1.0033181219100953\n",
            "Epoch 15, Loss: 0.9833211713631947\n",
            "Epoch 16, Loss: 0.9654710116386414\n",
            "Epoch 17, Loss: 0.946650700767835\n",
            "Epoch 18, Loss: 0.9321301391124726\n",
            "Epoch 19, Loss: 0.9166532247463862\n",
            "Epoch 20, Loss: 0.9057310500542323\n",
            "Epoch 21, Loss: 0.8939989454348882\n",
            "Epoch 22, Loss: 0.882178773621718\n",
            "Epoch 23, Loss: 0.8706949204007784\n",
            "Epoch 24, Loss: 0.8584521907567978\n",
            "Epoch 25, Loss: 0.8510625962018966\n",
            "Epoch 26, Loss: 0.8414546262423197\n",
            "Epoch 27, Loss: 0.83161649086078\n",
            "Epoch 28, Loss: 0.8242592129508655\n",
            "Epoch 29, Loss: 0.817155737777551\n",
            "Epoch 30, Loss: 0.8075070157845815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-15 17:42:12,906] Trial 3 finished with value: 78.66666666666667 and parameters: {'num_hidden_layers': 1, 'neurons_per_layer': 128, 'epochs': 30, 'learning_rate': 5.083940707389007e-05, 'dropout_rate': 0.5, 'batch_size': 32, 'optimizer': 'SGD', 'weight_decay': 2.5365674345343446e-05}. Best is trial 1 with value: 88.80833333333334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8999913387298584\n",
            "Epoch 2, Loss: 0.6098714534044266\n",
            "Epoch 3, Loss: 0.5558935481707256\n",
            "Epoch 4, Loss: 0.5317393318414688\n",
            "Epoch 5, Loss: 0.5076516150037448\n",
            "Epoch 6, Loss: 0.49750906850894294\n",
            "Epoch 7, Loss: 0.48352980315685273\n",
            "Epoch 8, Loss: 0.47510433769226074\n",
            "Epoch 9, Loss: 0.46786819249391554\n",
            "Epoch 10, Loss: 0.4648853273789088\n",
            "Epoch 11, Loss: 0.45619684445858\n",
            "Epoch 12, Loss: 0.4488088257908821\n",
            "Epoch 13, Loss: 0.4508116141756376\n",
            "Epoch 14, Loss: 0.44266316546996437\n",
            "Epoch 15, Loss: 0.43821798849105836\n",
            "Epoch 16, Loss: 0.4336351941227913\n",
            "Epoch 17, Loss: 0.4350296621521314\n",
            "Epoch 18, Loss: 0.42979834268490474\n",
            "Epoch 19, Loss: 0.4254180289109548\n",
            "Epoch 20, Loss: 0.4222724462548892\n",
            "Epoch 21, Loss: 0.4220566095113754\n",
            "Epoch 22, Loss: 0.4213402707775434\n",
            "Epoch 23, Loss: 0.41701694564024605\n",
            "Epoch 24, Loss: 0.41861018085479734\n",
            "Epoch 25, Loss: 0.4098053230047226\n",
            "Epoch 26, Loss: 0.4091256971557935\n",
            "Epoch 27, Loss: 0.41118740713596347\n",
            "Epoch 28, Loss: 0.4048536069194476\n",
            "Epoch 29, Loss: 0.40346251340707145\n",
            "Epoch 30, Loss: 0.4061780154109001\n",
            "Epoch 31, Loss: 0.40112658981482185\n",
            "Epoch 32, Loss: 0.40259778585036593\n",
            "Epoch 33, Loss: 0.39887059621016185\n",
            "Epoch 34, Loss: 0.3976601087252299\n",
            "Epoch 35, Loss: 0.39835925871133804\n",
            "Epoch 36, Loss: 0.3945619637568792\n",
            "Epoch 37, Loss: 0.3906703520019849\n",
            "Epoch 38, Loss: 0.39055569178859395\n",
            "Epoch 39, Loss: 0.3892857607404391\n",
            "Epoch 40, Loss: 0.3902591444452604\n",
            "Epoch 41, Loss: 0.38589899269739786\n",
            "Epoch 42, Loss: 0.3852339441180229\n",
            "Epoch 43, Loss: 0.38420588743686673\n",
            "Epoch 44, Loss: 0.38299662274122237\n",
            "Epoch 45, Loss: 0.3841651391585668\n",
            "Epoch 46, Loss: 0.3832162775794665\n",
            "Epoch 47, Loss: 0.37998336599270505\n",
            "Epoch 48, Loss: 0.3829893127083778\n",
            "Epoch 49, Loss: 0.3799975890914599\n",
            "Epoch 50, Loss: 0.37755840555826825\n",
            "Epoch 51, Loss: 0.3804390860795975\n",
            "Epoch 52, Loss: 0.37754562187194823\n",
            "Epoch 53, Loss: 0.37373130176464714\n",
            "Epoch 54, Loss: 0.37632199492057167\n",
            "Epoch 55, Loss: 0.3746062918504079\n",
            "Epoch 56, Loss: 0.3714769647916158\n",
            "Epoch 57, Loss: 0.3692604027787844\n",
            "Epoch 58, Loss: 0.3725845114588737\n",
            "Epoch 59, Loss: 0.37044909518957136\n",
            "Epoch 60, Loss: 0.36882575913270316\n",
            "Epoch 61, Loss: 0.37278742482264837\n",
            "Epoch 62, Loss: 0.3698550126949946\n",
            "Epoch 63, Loss: 0.3664760587414106\n",
            "Epoch 64, Loss: 0.3670299989978472\n",
            "Epoch 65, Loss: 0.36795678474505744\n",
            "Epoch 66, Loss: 0.3650080208778381\n",
            "Epoch 67, Loss: 0.36896168506145477\n",
            "Epoch 68, Loss: 0.36557290877898535\n",
            "Epoch 69, Loss: 0.3638779154419899\n",
            "Epoch 70, Loss: 0.3608028842409452\n",
            "Epoch 71, Loss: 0.3629863797823588\n",
            "Epoch 72, Loss: 0.36031508892774583\n",
            "Epoch 73, Loss: 0.36097100769480067\n",
            "Epoch 74, Loss: 0.3578538850148519\n",
            "Epoch 75, Loss: 0.3600961033900579\n",
            "Epoch 76, Loss: 0.3551965014735858\n",
            "Epoch 77, Loss: 0.354548617819945\n",
            "Epoch 78, Loss: 0.35868338015675544\n",
            "Epoch 79, Loss: 0.35311492772897085\n",
            "Epoch 80, Loss: 0.35836008248726525\n",
            "Epoch 81, Loss: 0.354680953592062\n",
            "Epoch 82, Loss: 0.352245692829291\n",
            "Epoch 83, Loss: 0.35486750626564023\n",
            "Epoch 84, Loss: 0.35658315430084864\n",
            "Epoch 85, Loss: 0.3570641188422839\n",
            "Epoch 86, Loss: 0.35297357922792433\n",
            "Epoch 87, Loss: 0.35387180849909783\n",
            "Epoch 88, Loss: 0.3512622203230858\n",
            "Epoch 89, Loss: 0.3490572338898977\n",
            "Epoch 90, Loss: 0.3524617835978667\n",
            "Epoch 91, Loss: 0.35094312645991643\n",
            "Epoch 92, Loss: 0.3471010603109996\n",
            "Epoch 93, Loss: 0.34737804663181304\n",
            "Epoch 94, Loss: 0.3513925379316012\n",
            "Epoch 95, Loss: 0.3472808914979299\n",
            "Epoch 96, Loss: 0.34604141928752263\n",
            "Epoch 97, Loss: 0.34841953102747597\n",
            "Epoch 98, Loss: 0.3460991648832957\n",
            "Epoch 99, Loss: 0.34752611444393794\n",
            "Epoch 100, Loss: 0.34555316559473676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-15 17:44:35,991] Trial 4 finished with value: 86.73333333333333 and parameters: {'num_hidden_layers': 1, 'neurons_per_layer': 32, 'epochs': 100, 'learning_rate': 0.01621685816067072, 'dropout_rate': 0.30000000000000004, 'batch_size': 64, 'optimizer': 'SGD', 'weight_decay': 0.00032157629766401574}. Best is trial 1 with value: 88.80833333333334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8915095737377803\n",
            "Epoch 2, Loss: 0.6288092111249765\n",
            "Epoch 3, Loss: 0.5849453803201516\n",
            "Epoch 4, Loss: 0.5604249959588051\n",
            "Epoch 5, Loss: 0.5421941731174786\n",
            "Epoch 6, Loss: 0.5245662169158458\n",
            "Epoch 7, Loss: 0.5165016142725944\n",
            "Epoch 8, Loss: 0.5095216555595398\n",
            "Epoch 9, Loss: 0.504663629492124\n",
            "Epoch 10, Loss: 0.4884319495558739\n",
            "Epoch 11, Loss: 0.49185765970746675\n",
            "Epoch 12, Loss: 0.48190040797988576\n",
            "Epoch 13, Loss: 0.48181168217460313\n",
            "Epoch 14, Loss: 0.47501459665596485\n",
            "Epoch 15, Loss: 0.47181904727220536\n",
            "Epoch 16, Loss: 0.4675127329727014\n",
            "Epoch 17, Loss: 0.4668754889965057\n",
            "Epoch 18, Loss: 0.46596985443433125\n",
            "Epoch 19, Loss: 0.4602501332362493\n",
            "Epoch 20, Loss: 0.45425656329095365\n",
            "Epoch 21, Loss: 0.4562406214972337\n",
            "Epoch 22, Loss: 0.4524307096699874\n",
            "Epoch 23, Loss: 0.45336666694283484\n",
            "Epoch 24, Loss: 0.4514449192037185\n",
            "Epoch 25, Loss: 0.4460394856631756\n",
            "Epoch 26, Loss: 0.4485663784344991\n",
            "Epoch 27, Loss: 0.4457905806154013\n",
            "Epoch 28, Loss: 0.44647400175035\n",
            "Epoch 29, Loss: 0.44258733809987705\n",
            "Epoch 30, Loss: 0.4460864820331335\n",
            "Epoch 31, Loss: 0.4405940536608299\n",
            "Epoch 32, Loss: 0.44291292935609816\n",
            "Epoch 33, Loss: 0.43997301885982354\n",
            "Epoch 34, Loss: 0.44270770795643327\n",
            "Epoch 35, Loss: 0.43825259976585706\n",
            "Epoch 36, Loss: 0.43734355934957664\n",
            "Epoch 37, Loss: 0.4377811154375474\n",
            "Epoch 38, Loss: 0.44079096941153206\n",
            "Epoch 39, Loss: 0.436783728475372\n",
            "Epoch 40, Loss: 0.4356726313382387\n",
            "Epoch 41, Loss: 0.4320613418122133\n",
            "Epoch 42, Loss: 0.4341970441490412\n",
            "Epoch 43, Loss: 0.43435600376625855\n",
            "Epoch 44, Loss: 0.43255609664320943\n",
            "Epoch 45, Loss: 0.4311423932959636\n",
            "Epoch 46, Loss: 0.4298862732301156\n",
            "Epoch 47, Loss: 0.4327198599129915\n",
            "Epoch 48, Loss: 0.43103735269606114\n",
            "Epoch 49, Loss: 0.428732622757554\n",
            "Epoch 50, Loss: 0.4291511139174302\n",
            "Epoch 51, Loss: 0.42578097302218276\n",
            "Epoch 52, Loss: 0.4264545340935389\n",
            "Epoch 53, Loss: 0.4251627986629804\n",
            "Epoch 54, Loss: 0.4249816794643799\n",
            "Epoch 55, Loss: 0.4249692482203245\n",
            "Epoch 56, Loss: 0.4245786108573278\n",
            "Epoch 57, Loss: 0.4244657059858243\n",
            "Epoch 58, Loss: 0.4263955336858829\n",
            "Epoch 59, Loss: 0.42365829934676486\n",
            "Epoch 60, Loss: 0.42360195542375245\n",
            "Epoch 61, Loss: 0.42202088264624277\n",
            "Epoch 62, Loss: 0.42602491074303783\n",
            "Epoch 63, Loss: 0.4256459281891584\n",
            "Epoch 64, Loss: 0.41982741327087086\n",
            "Epoch 65, Loss: 0.4241562421371539\n",
            "Epoch 66, Loss: 0.42317468308409056\n",
            "Epoch 67, Loss: 0.4194456323881944\n",
            "Epoch 68, Loss: 0.41808487256368\n",
            "Epoch 69, Loss: 0.4214429961591959\n",
            "Epoch 70, Loss: 0.41927386563519636\n",
            "Epoch 71, Loss: 0.4196038081695636\n",
            "Epoch 72, Loss: 0.41663477310538294\n",
            "Epoch 73, Loss: 0.41955830046037834\n",
            "Epoch 74, Loss: 0.4182260400603215\n",
            "Epoch 75, Loss: 0.41667433378100394\n",
            "Epoch 76, Loss: 0.4185774420897166\n",
            "Epoch 77, Loss: 0.4169978157629569\n",
            "Epoch 78, Loss: 0.41656200208266575\n",
            "Epoch 79, Loss: 0.41690415173768997\n",
            "Epoch 80, Loss: 0.4171738218218088\n",
            "Epoch 81, Loss: 0.4169012189855178\n",
            "Epoch 82, Loss: 0.41544611382484437\n",
            "Epoch 83, Loss: 0.41343492472171783\n",
            "Epoch 84, Loss: 0.4191371760716041\n",
            "Epoch 85, Loss: 0.4110761260588964\n",
            "Epoch 86, Loss: 0.4154970819503069\n",
            "Epoch 87, Loss: 0.4110788279821475\n",
            "Epoch 88, Loss: 0.4204269968072573\n",
            "Epoch 89, Loss: 0.41426808207233745\n",
            "Epoch 90, Loss: 0.41617226838568844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-15 17:50:50,943] Trial 5 finished with value: 87.81666666666666 and parameters: {'num_hidden_layers': 3, 'neurons_per_layer': 48, 'epochs': 90, 'learning_rate': 0.0006938451508248465, 'dropout_rate': 0.30000000000000004, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 4.1513255582650815e-05}. Best is trial 1 with value: 88.80833333333334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.7704097566604615\n",
            "Epoch 2, Loss: 1.5208694764773052\n",
            "Epoch 3, Loss: 1.4778862462838491\n",
            "Epoch 4, Loss: 1.4569678584734598\n",
            "Epoch 5, Loss: 1.4176298650105794\n",
            "Epoch 6, Loss: 1.4151618213653565\n",
            "Epoch 7, Loss: 1.406294753630956\n",
            "Epoch 8, Loss: 1.4186855211257934\n",
            "Epoch 9, Loss: 1.39974138434728\n",
            "Epoch 10, Loss: 1.4087440139452616\n",
            "Epoch 11, Loss: 1.4186006995836893\n",
            "Epoch 12, Loss: 1.4133282214800518\n",
            "Epoch 13, Loss: 1.4392187019983926\n",
            "Epoch 14, Loss: 1.4272698621749877\n",
            "Epoch 15, Loss: 1.4188706115086873\n",
            "Epoch 16, Loss: 1.4055766917069754\n",
            "Epoch 17, Loss: 1.4011516040166219\n",
            "Epoch 18, Loss: 1.403233081181844\n",
            "Epoch 19, Loss: 1.4175549171765645\n",
            "Epoch 20, Loss: 1.4074244662920634\n",
            "Epoch 21, Loss: 1.4028165767987568\n",
            "Epoch 22, Loss: 1.4126538048585255\n",
            "Epoch 23, Loss: 1.4178093260129292\n",
            "Epoch 24, Loss: 1.4045551280975341\n",
            "Epoch 25, Loss: 1.4140630427201588\n",
            "Epoch 26, Loss: 1.4087290824254355\n",
            "Epoch 27, Loss: 1.4142375276883443\n",
            "Epoch 28, Loss: 1.4246920496622721\n",
            "Epoch 29, Loss: 1.4092320014635722\n",
            "Epoch 30, Loss: 1.4196269256273906\n",
            "Epoch 31, Loss: 1.4281971122423809\n",
            "Epoch 32, Loss: 1.430441903114319\n",
            "Epoch 33, Loss: 1.422774378299713\n",
            "Epoch 34, Loss: 1.417949909210205\n",
            "Epoch 35, Loss: 1.4196033066908518\n",
            "Epoch 36, Loss: 1.4315312940279643\n",
            "Epoch 37, Loss: 1.433466645558675\n",
            "Epoch 38, Loss: 1.4303548227945964\n",
            "Epoch 39, Loss: 1.4335500866572062\n",
            "Epoch 40, Loss: 1.4348109707832337\n",
            "Epoch 41, Loss: 1.4312764123280843\n",
            "Epoch 42, Loss: 1.4376310998598734\n",
            "Epoch 43, Loss: 1.4290792892773947\n",
            "Epoch 44, Loss: 1.4308819292386372\n",
            "Epoch 45, Loss: 1.4285952135721842\n",
            "Epoch 46, Loss: 1.4411194307009378\n",
            "Epoch 47, Loss: 1.4523977204958598\n",
            "Epoch 48, Loss: 1.4452546071211496\n",
            "Epoch 49, Loss: 1.4405745615959167\n",
            "Epoch 50, Loss: 1.450383351167043\n",
            "Epoch 51, Loss: 1.4659201380411784\n",
            "Epoch 52, Loss: 1.4549100410143534\n",
            "Epoch 53, Loss: 1.4640149224599202\n",
            "Epoch 54, Loss: 1.4602776821454366\n",
            "Epoch 55, Loss: 1.4718194740613302\n",
            "Epoch 56, Loss: 1.4610941441853842\n",
            "Epoch 57, Loss: 1.4636453043619793\n",
            "Epoch 58, Loss: 1.4467283789316814\n",
            "Epoch 59, Loss: 1.4662035239537556\n",
            "Epoch 60, Loss: 1.4633942761421204\n",
            "Epoch 61, Loss: 1.461764983812968\n",
            "Epoch 62, Loss: 1.4671438706715902\n",
            "Epoch 63, Loss: 1.4551802905400595\n",
            "Epoch 64, Loss: 1.4554895825386047\n",
            "Epoch 65, Loss: 1.4578174654642742\n",
            "Epoch 66, Loss: 1.4629047220547995\n",
            "Epoch 67, Loss: 1.4605150904655457\n",
            "Epoch 68, Loss: 1.4669154376983642\n",
            "Epoch 69, Loss: 1.4566818469365437\n",
            "Epoch 70, Loss: 1.4683906262715658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-15 17:54:08,575] Trial 6 finished with value: 54.65 and parameters: {'num_hidden_layers': 5, 'neurons_per_layer': 96, 'epochs': 70, 'learning_rate': 0.0565050609280268, 'dropout_rate': 0.5, 'batch_size': 64, 'optimizer': 'RMSprop', 'weight_decay': 9.574671933622558e-05}. Best is trial 1 with value: 88.80833333333334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9627239445050557\n",
            "Epoch 2, Loss: 0.7522825832366944\n",
            "Epoch 3, Loss: 0.728454260190328\n",
            "Epoch 4, Loss: 0.7109876887003581\n",
            "Epoch 5, Loss: 0.7028116117318471\n",
            "Epoch 6, Loss: 0.6927333190441132\n",
            "Epoch 7, Loss: 0.6898318702379862\n",
            "Epoch 8, Loss: 0.6881693137486776\n",
            "Epoch 9, Loss: 0.6836293772856394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-15 17:54:22,777] Trial 7 finished with value: 82.36666666666666 and parameters: {'num_hidden_layers': 4, 'neurons_per_layer': 80, 'epochs': 10, 'learning_rate': 0.009746061997760468, 'dropout_rate': 0.5, 'batch_size': 128, 'optimizer': 'RMSprop', 'weight_decay': 4.5506047118325213e-05}. Best is trial 1 with value: 88.80833333333334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.6778009703159332\n",
            "Epoch 1, Loss: 2.2575606956481935\n",
            "Epoch 2, Loss: 2.1386311683654786\n",
            "Epoch 3, Loss: 2.059745486895243\n",
            "Epoch 4, Loss: 1.9988887519836427\n",
            "Epoch 5, Loss: 1.9534305159250895\n",
            "Epoch 6, Loss: 1.9140985940297444\n",
            "Epoch 7, Loss: 1.8854332898457844\n",
            "Epoch 8, Loss: 1.8560812877019246\n",
            "Epoch 9, Loss: 1.8319835920333862\n",
            "Epoch 10, Loss: 1.8055233373641968\n",
            "Epoch 11, Loss: 1.78498104763031\n",
            "Epoch 12, Loss: 1.7631713717778523\n",
            "Epoch 13, Loss: 1.7440631062189738\n",
            "Epoch 14, Loss: 1.7246189279556274\n",
            "Epoch 15, Loss: 1.7066784524917602\n",
            "Epoch 16, Loss: 1.68837260278066\n",
            "Epoch 17, Loss: 1.67284393119812\n",
            "Epoch 18, Loss: 1.6538245843251547\n",
            "Epoch 19, Loss: 1.6383330039978028\n",
            "Epoch 20, Loss: 1.6253142601648967\n",
            "Epoch 21, Loss: 1.60596799437205\n",
            "Epoch 22, Loss: 1.5912307856877645\n",
            "Epoch 23, Loss: 1.5766896006266276\n",
            "Epoch 24, Loss: 1.5621441532770792\n",
            "Epoch 25, Loss: 1.548713591893514\n",
            "Epoch 26, Loss: 1.5379614505767822\n",
            "Epoch 27, Loss: 1.5235387630462647\n",
            "Epoch 28, Loss: 1.5107482233047484\n",
            "Epoch 29, Loss: 1.4972288204828899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-15 17:54:54,784] Trial 8 finished with value: 69.49166666666666 and parameters: {'num_hidden_layers': 2, 'neurons_per_layer': 40, 'epochs': 30, 'learning_rate': 8.035186090204443e-05, 'dropout_rate': 0.2, 'batch_size': 128, 'optimizer': 'SGD', 'weight_decay': 3.1010295455822646e-05}. Best is trial 1 with value: 88.80833333333334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Loss: 1.4863474537531536\n",
            "Epoch 1, Loss: 0.9810717425942421\n",
            "Epoch 2, Loss: 0.725169744938612\n",
            "Epoch 3, Loss: 0.6590540001938741\n",
            "Epoch 4, Loss: 0.6219125208432476\n",
            "Epoch 5, Loss: 0.5897999877432982\n",
            "Epoch 6, Loss: 0.5795014652659496\n",
            "Epoch 7, Loss: 0.5679007366498311\n",
            "Epoch 8, Loss: 0.5498361547514796\n",
            "Epoch 9, Loss: 0.5403701269105077\n",
            "Epoch 10, Loss: 0.5249688339109222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-15 17:56:18,153] Trial 9 finished with value: 86.84166666666667 and parameters: {'num_hidden_layers': 4, 'neurons_per_layer': 128, 'epochs': 10, 'learning_rate': 0.014432972238741631, 'dropout_rate': 0.4, 'batch_size': 16, 'optimizer': 'SGD', 'weight_decay': 1.5424994640731916e-05}. Best is trial 1 with value: 88.80833333333334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.286612787246704\n",
            "Epoch 2, Loss: 2.2009249267578124\n",
            "Epoch 3, Loss: 2.151641866048177\n",
            "Epoch 4, Loss: 2.107329005877177\n",
            "Epoch 5, Loss: 2.065323424975077\n",
            "Epoch 6, Loss: 2.0278420095443725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-07-15 17:56:28,501] Trial 10 failed with parameters: {'num_hidden_layers': 4, 'neurons_per_layer': 16, 'epochs': 90, 'learning_rate': 1.3378360208554625e-05, 'dropout_rate': 0.2, 'batch_size': 128, 'optimizer': 'Adam', 'weight_decay': 0.0003000014016951423} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-20-1962216220.py\", line 38, in objective\n",
            "    for batch_features, batch_labels in train_loader:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 704, in __next__\n",
            "    with torch.autograd.profiler.record_function(self._profile_name):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py\", line 769, in __exit__\n",
            "    torch.ops.profiler._record_function_exit._RecordFunction(record)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 947, in __call__\n",
            "    if _must_dispatch_in_python(args, kwargs):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1001, in _must_dispatch_in_python\n",
            "    return pytree.tree_any(\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\", line 1230, in tree_any\n",
            "    return any(map(pred, flat_args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\", line 931, in tree_iter\n",
            "    yield from tree_iter(child, is_leaf=is_leaf)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\", line 927, in tree_iter\n",
            "    child_pytrees, _ = flatten_fn(tree)\n",
            "                       ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\", line 467, in _dict_flatten\n",
            "    return list(d.values()), list(d.keys())\n",
            "                             ^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-07-15 17:56:28,508] Trial 10 failed with value None.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Loss: 1.9926121775309245\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-22-1374290660.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \"\"\"\n\u001b[0;32m--> 489\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-20-1962216220.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtotal_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m      \u001b[0mbatch_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m      \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0;31m# When any inputs are FakeScriptObject, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;31m# skip c++ dispatcher and dispatch in python through _get_dispatch of python_dispatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m_must_dispatch_in_python\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m     return pytree.tree_any(\n\u001b[0m\u001b[1;32m   1002\u001b[0m         lambda obj: isinstance(\n\u001b[1;32m   1003\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_class_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFakeScriptObject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_any\u001b[0;34m(pred, tree, is_leaf)\u001b[0m\n\u001b[1;32m   1228\u001b[0m ) -> bool:\n\u001b[1;32m   1229\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_iter\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;31m# Recursively flatten the children\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchild_pytrees\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtree_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_iter\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mnode_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_node_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0mflatten_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSUPPORTED_NODES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0mchild_pytrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;31m# Recursively flatten the children\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36m_dict_flatten\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_dict_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mContext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3D8gQz03ngY",
        "outputId": "1f03748a-e579-4918-fbbf-cc36464109ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88.80833333333334"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeZBLGz63zKJ",
        "outputId": "bc83be3e-6c07-4be7-fbb0-47a49d739e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_hidden_layers': 4,\n",
              " 'neurons_per_layer': 72,\n",
              " 'epochs': 100,\n",
              " 'learning_rate': 5.817284058149812e-05,\n",
              " 'dropout_rate': 0.2,\n",
              " 'batch_size': 128,\n",
              " 'optimizer': 'Adam',\n",
              " 'weight_decay': 3.794627897715172e-05}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}